{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONSTER JOB POSTING ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_skills = [\n",
    "        \"Python\",\n",
    "        \"SQL\",\n",
    "        \"R\",\n",
    "        \"Spark\",\n",
    "        \"SAS\",\n",
    "        \"Javascript\",\n",
    "        \"SAAS\",\n",
    "        \"Hive\",\n",
    "        \"Scala\",\n",
    "        \"Excel\",\n",
    "        \"TensorFlow\",\n",
    "        \"C++\",\n",
    "        \"Azure\",\n",
    "        \"NoSQL\",\n",
    "        \"Linux\",\n",
    "        \"C\",\n",
    "        \"Matlab\",\n",
    "        \"Hadoop\",\n",
    "        \"Java\",\n",
    "        \"Tableau\",\n",
    "        \"AWS\",\n",
    "        \"Git\",\n",
    "        \"Javascript\",\n",
    "        \"Pig\",\n",
    "        \"Hbase\",\n",
    "        \"Google Cloud\",\n",
    "        \"Docker\",\n",
    "        \"NumPy\",\n",
    "        \"PyTorch\",\n",
    "        \"C#\",\n",
    "        \"SPSS\",\n",
    "        \"MySQL\",\n",
    "        \"Perl\",\n",
    "        \"Cassandra\",\n",
    "        \"MongoDB\",\n",
    "        \"GCP\",\n",
    "        \"Kubernetes\",\n",
    "        \"D3\",\n",
    "        \"Databricks\",\n",
    "        \"postgresql\",\n",
    "        \"Caffe\",\n",
    "        \"Airflow\",\n",
    "        \"Alteryx\",\n",
    "        \"BigQuery\",\n",
    "        \"Fastai\",\n",
    "]\n",
    "\n",
    "top_skills = [top_skill.upper() for top_skill in top_skills]\n",
    "top_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET COUNT OF JOB POSTINGS FOR EACH SKILL IN SKILLSET LIST ACROSS THE U.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_skill_count(top_skills):\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    import matplotlib.pyplot as plt\n",
    "    import calendar\n",
    "    from splinter import Browser\n",
    "    import datetime as dt\n",
    "    from datetime import date, datetime, timedelta\n",
    "    \n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Printing count of skills in Skillset list in U.S.A.....\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    \n",
    "    monster_list = []\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    #browser = Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "    for term in top_skills:\n",
    "        url = f'https://www.monster.com/jobs/search/?q=__22data-scientist__22-__22{term}__22'\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            soup = bs(r.text, 'html.parser')\n",
    "            count_str = soup.find('h2', class_=\"figure\").get_text()\n",
    "            numb = count_str.split()\n",
    "            monster_count = numb[0].replace(\"(\", \"\")\n",
    "            monster_list.append(monster_count)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    monster_list.append(0) # because no results for fastai. change to check if length matches first.\n",
    "    \n",
    "    df = pd.DataFrame(monster_list, index=top_skills, columns=[f'Monster Skill Count {date.today()}'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Printing count of skills in Skillset list in U.S.A.....\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 1), indices imply (45, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1690\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 1691\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 1), indices imply (45, 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-64448ebc2afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_top_skill_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_skills\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-98df6ac39a96>\u001b[0m in \u001b[0;36mget_top_skill_count\u001b[0;34m(top_skills)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmonster_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# because no results for fastai. change to check if length matches first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonster_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_skills\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'Monster Skill Count {date.today()}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                     mgr = init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 451\u001b[0;31m                                        copy=copy)\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 1691\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 1), indices imply (45, 1)"
     ]
    }
   ],
   "source": [
    "get_top_skill_count(top_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = get_top_skill_count(top_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monster Skill Count 2020-02-22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PYTHON</th>\n",
       "      <td>2958.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>2366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQL</th>\n",
       "      <td>2089.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPARK</th>\n",
       "      <td>1287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HADOOP</th>\n",
       "      <td>1155.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Monster Skill Count 2020-02-22\n",
       "PYTHON                          2958.0\n",
       "R                               2366.0\n",
       "SQL                             2089.0\n",
       "SPARK                           1287.0\n",
       "HADOOP                          1155.0"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.apply(lambda x:x.astype(float), axis=1).sort_values(by='Monster Skill Count 2020-02-22', ascending=False)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf9c8ea1a1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Science Skillset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ranking.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = x.plot(kind='bar', figsize=(25,17)).set_title(\"Data Science Skillset\", size=30).get_figure().savefig('ranking.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING SKILLSET COUNT TO POSTGRES DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlalchemy import create_engine , inspect\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "from sqlalchemy import Column, Integer, String, Float, DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = get_top_skill_count(top_skills).reset_index()#T.to_dict('list')\n",
    "skills.columns = [\"Skill\", \"Count\"]\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the posting class (the posting table that will be in the db)\n",
    "class skills_count(Base):\n",
    "    __tablename__ = 'Skill_counts'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Skill = Column(String(255))\n",
    "    Count = Column(String(255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to a SQLite database\n",
    "engine = create_engine('sqlite:///skills_count.db')\n",
    "conn = engine.connect()\n",
    "Base.metadata.create_all(engine)  #this should create the table based on the classes defined\n",
    "from sqlalchemy.orm import Session\n",
    "session = Session(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over df and add rows as records to table\n",
    "for i in skills.itertuples():\n",
    "    try:\n",
    "        record = Skill_counts(Skill = i.Skill, Count = i.Count)\n",
    "        session.add(record)\n",
    "        session.commit()\n",
    " \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST TO SEE IF IT WAS LOADED CORRECTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['posting', 'skill_count', 'skill_counts']"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspector = inspect(engine)\n",
    "inspector.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id INTEGER\n",
      "Skill VARCHAR(255)\n",
      "Count INTEGER\n"
     ]
    }
   ],
   "source": [
    "columns = inspector.get_columns('skill_counts')\n",
    "for column in columns:\n",
    "    print(column[\"name\"], column[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON\n",
      "SQL\n",
      "R\n",
      "SPARK\n",
      "SAS\n",
      "JAVASCRIPT\n",
      "SAAS\n",
      "HIVE\n",
      "SCALA\n",
      "EXCEL\n",
      "TENSORFLOW\n",
      "C++\n",
      "AZURE\n",
      "NOSQL\n",
      "LINUX\n",
      "C\n",
      "MATLAB\n",
      "HADOOP\n",
      "JAVA\n",
      "TABLEAU\n",
      "AWS\n",
      "GIT\n",
      "JAVASCRIPT\n",
      "PIG\n",
      "HBASE\n",
      "GOOGLE CLOUD\n",
      "DOCKER\n",
      "NUMPY\n",
      "PYTORCH\n",
      "C#\n",
      "SPSS\n",
      "MYSQL\n",
      "PERL\n",
      "CASSANDRA\n",
      "MONGODB\n",
      "GCP\n",
      "KUBERNETES\n",
      "D3\n",
      "DATABRICKS\n",
      "POSTGRESQL\n",
      "CAFFE\n",
      "AIRFLOW\n",
      "ALTERYX\n",
      "BIGQUERY\n",
      "FASTAI\n"
     ]
    }
   ],
   "source": [
    "skill_count = session.query(skills_count)\n",
    "for skill in skill_count:\n",
    "    print(skill.Skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  RESET DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to clear out the db when necessary\n",
    "\n",
    "#Base.metadata.drop_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET JOB POSTING DATA FOR ANY DATA SCIENCE SKILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skill_data():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    import calendar\n",
    "    import datetime as dt\n",
    "    from datetime import date, datetime, timedelta\n",
    "    import requests\n",
    "    import re\n",
    "    from splinter import Browser\n",
    "\n",
    "    term = input(\"Hi there! What data science skill would you like to find out about today?    \")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    n = int(input(\"How many job postings would you like to see?    \"))\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    browser = Browser(\"chrome\", **executable_path, headless=False)\n",
    "    top_skills_data = []\n",
    "\n",
    "    term = \"python\"\n",
    "\n",
    "    week_day = []\n",
    "    posted = []\n",
    "    datee = []\n",
    "    title = []\n",
    "    company = []\n",
    "    location = []\n",
    "    city = []\n",
    "    state = []\n",
    "    zipcode = []\n",
    "    scraped_data = {}\n",
    "    title_resultss = []\n",
    "    company_resultss = []\n",
    "    location_resultss = []\n",
    "    date_resultss = []\n",
    "\n",
    "    url = f'https://www.monster.com/jobs/search/?q=__22data-scientist__22-__22{term}__22'\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    try:\n",
    "        browser.visit(url)\n",
    "\n",
    "\n",
    "        while i <= n:\n",
    "            try:\n",
    "\n",
    "                title_path = f'//*[@id=\"SearchResults\"]/section[{i}]/div/div[2]/header/h2/a'\n",
    "                title_results = browser.find_by_xpath(title_path)\n",
    "                \n",
    "                if title_results:\n",
    "                    title_resultss.append(title_results)\n",
    "\n",
    "                    date_path = f'//*[@id=\"SearchResults\"]/section[{i}]/div/div[3]/time'\n",
    "                    date_results = browser.find_by_xpath(date_path)\n",
    "                    date_resultss.append(date_results)\n",
    "\n",
    "                    company_path = f'//*[@id=\"SearchResults\"]/section[{i}]/div/div[2]/div[1]/span'\n",
    "                    company_results = browser.find_by_xpath(company_path)\n",
    "                    company_resultss.append(company_results)\n",
    "\n",
    "                    location_path = f'//*[@id=\"SearchResults\"]/section[{i}]/div/div[2]/div[2]/span'\n",
    "                    location_results = browser.find_by_xpath(location_path)\n",
    "                    location_resultss.append(location_results)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "            i += 1\n",
    "\n",
    "        for i in list(range(len(title_resultss))):\n",
    "            try:\n",
    "                title.append(title_resultss[i].text)\n",
    "\n",
    "                posted.append(date_resultss[i].text)\n",
    "\n",
    "                company.append(company_resultss[i].text)\n",
    "\n",
    "                if \"today\" in date_resultss[i].text:\n",
    "                    datee.append(date.today().strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "                    week_day.append(calendar.day_name[date.today().weekday()])\n",
    "                else:\n",
    "                    datee.append((date.today()-timedelta(days=int(re.findall(f'([0-9]+).+',date_resultss[i].text)[0]))).strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "                    week_day.append(calendar.day_name[(date.today()-timedelta(days=int(re.findall(f'([0-9]+).+',date_resultss[i].text)[0]))).weekday()])\n",
    "\n",
    "                location.append(location_resultss[i].text)\n",
    "\n",
    "                city.append(location_resultss[i].text.split(',')[0])\n",
    "\n",
    "                state.append(location_resultss[i].text.split(',')[1].split(\" \")[1])\n",
    "\n",
    "                try:\n",
    "\n",
    "                    zipcode.append(location_resultss[i].text.split(',')[1].split(\" \")[2])\n",
    "\n",
    "                except:\n",
    "                    zipcode.append(\" \")\n",
    "\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "\n",
    "        scraped_data['date'] = datee\n",
    "        scraped_data['posted'] = posted\n",
    "        scraped_data['week_day'] = week_day\n",
    "        scraped_data['title'] = title\n",
    "        scraped_data['company'] = company\n",
    "        #scraped_data['location'] = location\n",
    "        scraped_data['city'] = city\n",
    "        scraped_data['state'] = state\n",
    "        #scraped_data['zipcode'] = zipcode\n",
    "        top_skills_data.append(scraped_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    browser.quit()\n",
    "    \n",
    "    return pd.DataFrame(top_skills_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! What data science skill would you like to find out about today?    python\n",
      "--------------------------------------------------------------------------------\n",
      "How many job postings would you like to see?    44\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = get_skill_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>posted</th>\n",
       "      <th>week_day</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02/18/2020</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Data Scientist - Machine Learning, Deep Learni...</td>\n",
       "      <td>CyberCoders</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/22/2020</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Data Analytics Architect/Senior Data Scientist...</td>\n",
       "      <td>Vital Tech Solutions</td>\n",
       "      <td>Green Bay</td>\n",
       "      <td>WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/18/2020</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Data Scientist (NLP &amp; Python)</td>\n",
       "      <td>Randstad Technologies</td>\n",
       "      <td>Pennington</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02/19/2020</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Python Fintech Engineer / Developer and Data S...</td>\n",
       "      <td>Kavyos Consulting Inc</td>\n",
       "      <td>NYC</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/19/2020</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Data Scientist with Hands on in Python, 2 week...</td>\n",
       "      <td>Prodapt North America</td>\n",
       "      <td>Ashburn</td>\n",
       "      <td>VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02/18/2020</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Sr. Data Scientist - Python, Machine Learning,...</td>\n",
       "      <td>CyberCoders</td>\n",
       "      <td>Raleigh</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02/16/2020</td>\n",
       "      <td>7 days ago</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>NLP Software Engineer (HOLDING)</td>\n",
       "      <td>Fidelity TalentSource</td>\n",
       "      <td>Merrimack</td>\n",
       "      <td>NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02/23/2020</td>\n",
       "      <td>Posted today</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Sr Data Scientist - Broomfield, CO</td>\n",
       "      <td>CenturyLink</td>\n",
       "      <td>BROOMFIELD</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02/23/2020</td>\n",
       "      <td>Posted today</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>The Hartford</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>02/22/2020</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Senior Data Engineer (HealthTech Company)</td>\n",
       "      <td>Grow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>02/20/2020</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Data Scientists</td>\n",
       "      <td>Advantage Resourcing</td>\n",
       "      <td>Dearborn</td>\n",
       "      <td>MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>02/19/2020</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Net 2 Source</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>02/14/2020</td>\n",
       "      <td>9 days ago</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Director Data Analytics</td>\n",
       "      <td>Honeywell</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>02/14/2020</td>\n",
       "      <td>9 days ago</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Sedona Technologies Government Services</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>01/30/2020</td>\n",
       "      <td>24 days ago</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>AI Data Engineer</td>\n",
       "      <td>Sysazzle</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>01/24/2020</td>\n",
       "      <td>+30 days ago</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Data Science Engineer IV (E4)</td>\n",
       "      <td>0001 Applied Materials, Inc</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>02/23/2020</td>\n",
       "      <td>Posted today</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>InfoVision</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>02/22/2020</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Principle Data Scientist</td>\n",
       "      <td>Shenley Recruitment</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>02/22/2020</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>BI Analyst / Senior BI Analyst - Smyrna, GA</td>\n",
       "      <td>Masco HD Support Services, LLC</td>\n",
       "      <td>Smyrna</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>02/22/2020</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Henkel</td>\n",
       "      <td>Rocky Hill</td>\n",
       "      <td>CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>02/22/2020</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>BI Analyst / Senior BI Analyst - Mooresville, NC</td>\n",
       "      <td>Masco Retail Sales Support, Inc.</td>\n",
       "      <td>Mooresville</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02/21/2020</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Data Scientist 3</td>\n",
       "      <td>Collabera</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>02/20/2020</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Data Scientist (Pharmaceutical Manufacturing)</td>\n",
       "      <td>Company Confidential</td>\n",
       "      <td>West Point</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>02/19/2020</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Big Data Developer</td>\n",
       "      <td>Mastech Digital</td>\n",
       "      <td>Orangeburg</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>02/18/2020</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Monster</td>\n",
       "      <td>Weston</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date        posted   week_day  \\\n",
       "0   02/18/2020    5 days ago    Tuesday   \n",
       "1   02/22/2020     1 day ago   Saturday   \n",
       "2   02/18/2020    5 days ago    Tuesday   \n",
       "3   02/19/2020    4 days ago  Wednesday   \n",
       "4   02/19/2020    4 days ago  Wednesday   \n",
       "5   02/18/2020    5 days ago    Tuesday   \n",
       "6   02/16/2020    7 days ago     Sunday   \n",
       "7   02/23/2020  Posted today     Sunday   \n",
       "8   02/23/2020  Posted today     Sunday   \n",
       "9   02/22/2020     1 day ago   Saturday   \n",
       "10  02/20/2020    3 days ago   Thursday   \n",
       "11  02/19/2020    4 days ago  Wednesday   \n",
       "12  02/14/2020    9 days ago     Friday   \n",
       "13  02/14/2020    9 days ago     Friday   \n",
       "14  01/30/2020   24 days ago   Thursday   \n",
       "15  01/24/2020  +30 days ago     Friday   \n",
       "16  02/23/2020  Posted today     Sunday   \n",
       "17  02/22/2020     1 day ago   Saturday   \n",
       "18  02/22/2020     1 day ago   Saturday   \n",
       "19  02/22/2020     1 day ago   Saturday   \n",
       "20  02/22/2020     1 day ago   Saturday   \n",
       "21  02/21/2020    2 days ago     Friday   \n",
       "22  02/20/2020    3 days ago   Thursday   \n",
       "23  02/19/2020    4 days ago  Wednesday   \n",
       "24  02/18/2020    5 days ago    Tuesday   \n",
       "\n",
       "                                                title  \\\n",
       "0   Data Scientist - Machine Learning, Deep Learni...   \n",
       "1   Data Analytics Architect/Senior Data Scientist...   \n",
       "2                       Data Scientist (NLP & Python)   \n",
       "3   Python Fintech Engineer / Developer and Data S...   \n",
       "4   Data Scientist with Hands on in Python, 2 week...   \n",
       "5   Sr. Data Scientist - Python, Machine Learning,...   \n",
       "6                     NLP Software Engineer (HOLDING)   \n",
       "7                  Sr Data Scientist - Broomfield, CO   \n",
       "8                                Senior Data Engineer   \n",
       "9           Senior Data Engineer (HealthTech Company)   \n",
       "10                                    Data Scientists   \n",
       "11                                   Machine Learning   \n",
       "12                            Director Data Analytics   \n",
       "13                                     Data Scientist   \n",
       "14                                   AI Data Engineer   \n",
       "15                      Data Science Engineer IV (E4)   \n",
       "16                                     Data Scientist   \n",
       "17                           Principle Data Scientist   \n",
       "18        BI Analyst / Senior BI Analyst - Smyrna, GA   \n",
       "19                                     Data Scientist   \n",
       "20   BI Analyst / Senior BI Analyst - Mooresville, NC   \n",
       "21                                   Data Scientist 3   \n",
       "22      Data Scientist (Pharmaceutical Manufacturing)   \n",
       "23                                 Big Data Developer   \n",
       "24                              Senior Data Scientist   \n",
       "\n",
       "                                    company            city state  \n",
       "0                               CyberCoders       San Diego    CA  \n",
       "1                      Vital Tech Solutions       Green Bay    WI  \n",
       "2                     Randstad Technologies      Pennington    NJ  \n",
       "3                     Kavyos Consulting Inc             NYC    NY  \n",
       "4                     Prodapt North America         Ashburn    VA  \n",
       "5                               CyberCoders         Raleigh    NC  \n",
       "6                     Fidelity TalentSource       Merrimack    NH  \n",
       "7                               CenturyLink      BROOMFIELD    CO  \n",
       "8                              The Hartford        Hartford    CT  \n",
       "9                                      Grow       Manhattan    NY  \n",
       "10                     Advantage Resourcing        Dearborn    MI  \n",
       "11                             Net 2 Source           Plano    TX  \n",
       "12                                Honeywell       Charlotte    NC  \n",
       "13  Sedona Technologies Government Services           Tampa    FL  \n",
       "14                                 Sysazzle          Denver    CO  \n",
       "15              0001 Applied Materials, Inc  Salt Lake City    UT  \n",
       "16                               InfoVision           Plano    TX  \n",
       "17                      Shenley Recruitment    Philadelphia    PA  \n",
       "18           Masco HD Support Services, LLC          Smyrna    GA  \n",
       "19                                   Henkel      Rocky Hill    CT  \n",
       "20         Masco Retail Sales Support, Inc.     Mooresville    NC  \n",
       "21                                Collabera         Phoenix    AZ  \n",
       "22                     Company Confidential      West Point    PA  \n",
       "23                          Mastech Digital      Orangeburg    NY  \n",
       "24                                  Monster          Weston    MA  "
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING JOB POSTINGS TO MONGO DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.new_job_postings\n",
    "collection = db.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.insert_one(x.to_dict())\n",
    "#a = x.set_index('date').T.to_dict(\"list\")\n",
    "a = x.to_dict(\"list\")\n",
    "a\n",
    "key = list(a.keys())\n",
    "value = list(a.values())\n",
    "for i in range(len(key)):\n",
    "    collection.insert_one({key[i]:value[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5e521621693cb274859230cf'), 'date': ['02/18/2020', '02/22/2020', '02/18/2020', '02/19/2020', '02/19/2020', '02/18/2020', '02/16/2020', '02/23/2020', '02/23/2020', '02/22/2020', '02/20/2020', '02/19/2020', '02/14/2020', '02/14/2020', '01/30/2020', '01/24/2020', '02/23/2020', '02/22/2020', '02/22/2020', '02/22/2020', '02/22/2020', '02/21/2020', '02/20/2020', '02/19/2020', '02/18/2020']}\n",
      "{'_id': ObjectId('5e521621693cb274859230d0'), 'posted': ['5 days ago', '1 day ago', '5 days ago', '4 days ago', '4 days ago', '5 days ago', '7 days ago', 'Posted today', 'Posted today', '1 day ago', '3 days ago', '4 days ago', '9 days ago', '9 days ago', '24 days ago', '+30 days ago', 'Posted today', '1 day ago', '1 day ago', '1 day ago', '1 day ago', '2 days ago', '3 days ago', '4 days ago', '5 days ago']}\n",
      "{'_id': ObjectId('5e521621693cb274859230d1'), 'week_day': ['Tuesday', 'Saturday', 'Tuesday', 'Wednesday', 'Wednesday', 'Tuesday', 'Sunday', 'Sunday', 'Sunday', 'Saturday', 'Thursday', 'Wednesday', 'Friday', 'Friday', 'Thursday', 'Friday', 'Sunday', 'Saturday', 'Saturday', 'Saturday', 'Saturday', 'Friday', 'Thursday', 'Wednesday', 'Tuesday']}\n",
      "{'_id': ObjectId('5e521621693cb274859230d2'), 'title': ['Data Scientist - Machine Learning, Deep Learning, Python', 'Data Analytics Architect/Senior Data Scientist-R/RStudio/Python/ML/NL/AI', 'Data Scientist (NLP & Python)', 'Python Fintech Engineer / Developer and Data Scientist', 'Data Scientist with Hands on in Python, 2 weeks joining preferred', 'Sr. Data Scientist - Python, Machine Learning, Biometrics', 'NLP Software Engineer (HOLDING)', 'Sr Data Scientist - Broomfield, CO', 'Senior Data Engineer', 'Senior Data Engineer (HealthTech Company)', 'Data Scientists', 'Machine Learning', 'Director Data Analytics', 'Data Scientist', 'AI Data Engineer', 'Data Science Engineer IV (E4)', 'Data Scientist', 'Principle Data Scientist', 'BI Analyst / Senior BI Analyst - Smyrna, GA', 'Data Scientist', 'BI Analyst / Senior BI Analyst - Mooresville, NC', 'Data Scientist 3', 'Data Scientist (Pharmaceutical Manufacturing)', 'Big Data Developer', 'Senior Data Scientist']}\n",
      "{'_id': ObjectId('5e521621693cb274859230d3'), 'company': ['CyberCoders', 'Vital Tech Solutions', 'Randstad Technologies', 'Kavyos Consulting Inc', 'Prodapt North America', 'CyberCoders', 'Fidelity TalentSource', 'CenturyLink', 'The Hartford', 'Grow', 'Advantage Resourcing', 'Net 2 Source', 'Honeywell', 'Sedona Technologies Government Services', 'Sysazzle', '0001 Applied Materials, Inc', 'InfoVision', 'Shenley Recruitment', 'Masco HD Support Services, LLC', 'Henkel', 'Masco Retail Sales Support, Inc.', 'Collabera', 'Company Confidential', 'Mastech Digital', 'Monster']}\n",
      "{'_id': ObjectId('5e521621693cb274859230d4'), 'city': ['San Diego', 'Green Bay', 'Pennington', 'NYC', 'Ashburn', 'Raleigh', 'Merrimack', 'BROOMFIELD', 'Hartford', 'Manhattan', 'Dearborn', 'Plano', 'Charlotte', 'Tampa', 'Denver', 'Salt Lake City', 'Plano', 'Philadelphia', 'Smyrna', 'Rocky Hill', 'Mooresville', 'Phoenix', 'West Point', 'Orangeburg', 'Weston']}\n",
      "{'_id': ObjectId('5e521621693cb274859230d5'), 'state': ['CA', 'WI', 'NJ', 'NY', 'VA', 'NC', 'NH', 'CO', 'CT', 'NY', 'MI', 'TX', 'NC', 'FL', 'CO', 'UT', 'TX', 'PA', 'GA', 'CT', 'NC', 'AZ', 'PA', 'NY', 'MA']}\n"
     ]
    }
   ],
   "source": [
    "results = collection.find()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaf7339001cfe34b54ae1ffacfd6447597"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
